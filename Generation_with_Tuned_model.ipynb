{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPh3ErJy3FRYNGQd6mmY8QV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/LLM_fine_tuning/blob/main/Generation_with_Tuned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook allows you to run the LoRA fine tuned model by Syllog directly on Google Colab.\n",
        "\n",
        "The model was tuned on topics relevant to HR professionals, in **Italian language**.\n",
        "\n",
        "Please use the **T4 GPU** runtime accelerator"
      ],
      "metadata": {
        "id": "6e93X-NMzzeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import dependencies and get GPU\n",
        "%%capture\n",
        "\n",
        "!pip install trl transformers datasets torch peft\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU bitsandbytes\n",
        "!pip install thefuzz\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig, PeftModel\n",
        "from thefuzz import fuzz\n",
        "\n",
        "# Might be removed in future\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "00LU2e610Tzj",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to load all the components needed to run the model:\n",
        "- The Bits and Bytes configuration for the quantization (needed because of resource availability)\n",
        "- The tuned model\n",
        "- The associated tokenizer\n",
        "- The pipeline used for the output generation\n",
        "\n",
        "Note that this is quite a lot of stuff, so don't worry if the loading takes a while"
      ],
      "metadata": {
        "id": "FQOv2Mv26iy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the configuration\n",
        "%%capture\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "\n",
        "# For the time being we load the model from Drive.\n",
        "# In the future, once we have a Syllog HuggingFace account, we will load the model from there\n",
        "PEFT_MODEL = '/content/gdrive/MyDrive/Syllog/full_results/tuned_model'\n",
        "\n",
        "# Perf configuration\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
        "\n",
        "# Pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y3YfIBZV1Ni8",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that everything is loaded, you only need to ask your prompt and wait for the model to answer!"
      ],
      "metadata": {
        "id": "kv9AISrq8V9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Input the prompt\n",
        "user_prompt = input(\"Ask me anything related to HR, but remember that I only understand Italian:\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZdrJgAJ27qhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below section, there is the call to the model, and the processing of the generated answer"
      ],
      "metadata": {
        "id": "jw86N42iALtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Here is were the magic happens\n",
        "%%capture\n",
        "##############################################\n",
        "#############     GENERATION     #############\n",
        "##############################################\n",
        "\n",
        "system_message = \"Sei un assistente AI utile e conciso. Rispondi in massimo cinque frasi, va bene anche usarne meno.\"\n",
        "\n",
        "prompt_template=f\"\"\"<|im_start|>Sistema: {system_message}<|im_end|>\n",
        "<|im_start|>Utente: {user_prompt}<|im_end|>\n",
        "<|im_start|>Assistente: \"\"\"\n",
        "\n",
        "# Call the pipeline also with args to be passed to the model\n",
        "sequences = pipe(\n",
        "    prompt_template,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=False,\n",
        "    return_full_text=False,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=0,\n",
        ")\n",
        "\n",
        "answer = sequences[0]['generated_text']\n",
        "\n",
        "##############################################\n",
        "#############     PROCESSING     #############\n",
        "##############################################\n",
        "\n",
        "# If there is the end tag, let's just consider what's before it\n",
        "if '<|im_end|>' in answer:\n",
        "  answer = answer.split('<|im_end|>')[0]\n",
        "\n",
        "# Then, we want to remove the numbers of the numbered item list\n",
        "answer = re.sub(r'\\d+\\.\\s*', '- ', answer)\n",
        "\n",
        "# Then, what we want  to do is to verify that each sentence generated by the model is not similar to the others\n",
        "# We want to discard the last element as the model will always close a sentence with a dot.\n",
        "# If no dot is present, it means that the generation was interrupted because of the max tokens limit\n",
        "sentences = re.split(r'[.?!:;]', answer.strip())\n",
        "\n",
        "if len(sentences[-1]) > 0:\n",
        "  answer = answer[:-len(sentences[-1])]\n",
        "\n",
        "# If there are multiple sentences, check that they are different from each other\n",
        "if len(sentences) > 1:\n",
        "  sentences = sentences[:-1]\n",
        "\n",
        "  # Build the Fuzzy matching matrix\n",
        "  size = len(sentences)\n",
        "  fuzz_match = np.zeros((size, size))\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    for j, compare in enumerate(sentences):\n",
        "      if sentence is compare:\n",
        "        continue\n",
        "      else:\n",
        "        score = fuzz.token_set_ratio(sentence,compare)\n",
        "        fuzz_match[i][j] = score\n",
        "\n",
        "  # Discard sentences with high score\n",
        "  max_score = np.max(fuzz_match)\n",
        "  argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "  while max_score > 80:\n",
        "    # Find the two matching sentences\n",
        "    i = argmax_score // size\n",
        "    j = argmax_score % size\n",
        "\n",
        "    # print(f'Size: {size}, argmax: {argmax_score}, i: {i}, j: {j}')\n",
        "\n",
        "    # out of the two, find the one with the highest average score (the sentence on average more similar to all the others)\n",
        "    if fuzz_match[i].mean() < fuzz_match[j].mean():\n",
        "      to_delete = j\n",
        "    else:\n",
        "      assert fuzz_match[i].mean() >= fuzz_match[j].mean()\n",
        "      to_delete = i\n",
        "\n",
        "    # Delete sentence from the fuzz match\n",
        "    fuzz_match = np.delete(fuzz_match, to_delete, axis=0)\n",
        "    fuzz_match = np.delete(fuzz_match, to_delete, axis=1)\n",
        "\n",
        "    # Since we are deleting one sentence, we need to reduce the size as well\n",
        "    size -= 1\n",
        "\n",
        "    # Delete sentence from sentences\n",
        "    sentences.pop(to_delete)\n",
        "\n",
        "\n",
        "    # Values for the new While cycle\n",
        "    max_score = np.max(fuzz_match)\n",
        "    argmax_score = np.argmax(fuzz_match)\n",
        "\n",
        "  output = ''\n",
        "\n",
        "  for sentence in sentences:\n",
        "    idx = answer.find(sentence)\n",
        "\n",
        "    if idx != -1 and idx + len(sentence) < len(answer):\n",
        "        punctuation = answer[idx + len(sentence)]\n",
        "        output += sentence.strip() + punctuation + '\\n'\n",
        "    else:\n",
        "        print(\"Substring not found or character after the substring does not exist.\")\n",
        "\n",
        "else:\n",
        "  assert len(sentences) == 1\n",
        "  output = sentences[0]"
      ],
      "metadata": {
        "id": "pE2wwz2R09FK",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A print out of the generated output"
      ],
      "metadata": {
        "id": "3Ln8JOQHAUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print output\n",
        "print(f'You asked the Model:\\n{user_prompt}\\n')\n",
        "print(f'and the Model responded:\\n{output}')"
      ],
      "metadata": {
        "id": "oaMw2sHN8vcY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}