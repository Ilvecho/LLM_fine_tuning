{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fVVbSi5EHJDlXMtSgniPpbG9i95VdH1P",
      "authorship_tag": "ABX9TyMfciE1BbBxyJ6ntyC1lu/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecho/Web-Scraping/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapy - web crawler"
      ],
      "metadata": {
        "id": "Fn7n_ZN09H8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Scrapy"
      ],
      "metadata": {
        "id": "B9vb5_2zbMn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj3pIClta0CH",
        "outputId": "45a8605e-25df-4bab-fc17-25d0b78de6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "from google.colab import files,drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the spider to crawl the desired URLs"
      ],
      "metadata": {
        "id": "-oeYbgGFcgLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HrSpider(scrapy.Spider):\n",
        "  name = 'hr_spider'\n",
        "\n",
        "  def start_requests(self):\n",
        "    urls = ['https://www.getimpactly.com/post/hr-compliance-checklist',\n",
        "            'https://mariopeshev.com/4-ps-employee-relations-conflict-management/']\n",
        "\n",
        "    for url in urls:\n",
        "      yield scrapy.Request(url=url, callback=self.parse)\n",
        "\n",
        "  def parse(self, response):\n",
        "    output_text = ''\n",
        "    # Extract the content of interest\n",
        "    paragraphs = response.css('p, li::text, h2::text').extract()\n",
        "\n",
        "    # Stitch together everything\n",
        "    for content in paragraphs:\n",
        "      output_text = output_text + '\\n' + content\n",
        "\n",
        "    # Do the parsing\n",
        "    output_text = output_text.replace(\"<p>\",\"\")\n",
        "    output_text = output_text.replace(\"</p>\",\"\")\n",
        "    output_text = re.sub(r'<a href=\"[^\"]*\">', '', output_text)\n",
        "    output_text = output_text.replace(\"</a>\",\"\")\n",
        "\n",
        "    # Save the text in a file\n",
        "    with open(r'/content/drive/MyDrive/hr_content.txt', 'w') as text_file:\n",
        "      text_file.write(output_text)\n",
        "      text_file.close()\n"
      ],
      "metadata": {
        "id": "ljh3wXNGcjs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = CrawlerProcess()  # Look what Scrapy settings are # settings={'FEEDS': {'item.txt': {'format': 'txt'}}}\n",
        "process.crawl(HrSpider)\n",
        "process.start()"
      ],
      "metadata": {
        "id": "fC1ZNABUe4AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CwfNyLjfPsr",
        "outputId": "ff609847-46c3-49b0-970b-84dff7c9e899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DeferredList at 0x7a8ceb082fe0 current result: []>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beautiful Soup for HTML parsing"
      ],
      "metadata": {
        "id": "0evtdP-K9PNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "GC1D5qvki9UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_max_text_element(tag):\n",
        "    max_text_length = 0\n",
        "    max_text_element = None\n",
        "    length_list = []\n",
        "\n",
        "    for child in tag.find_all(recursive=False):\n",
        "      # Find the direct children only, not nested elements\n",
        "      text_length = len(child.get_text(strip=True))\n",
        "      length_list.append(text_length)\n",
        "      if text_length > max_text_length:\n",
        "        max_text_length = text_length\n",
        "        max_text_element = child\n",
        "\n",
        "    length_list = np.array(length_list) / max_text_length\n",
        "    print(length_list)\n",
        "    length_list[np.where(length_list == 1)] = 0\n",
        "\n",
        "    if (length_list > 0.10).any():\n",
        "\n",
        "      # Keep only the meaningful elements\n",
        "      for child in tag.find_all(recursive=False):\n",
        "        text_length = len(child.get_text(strip=True))\n",
        "        if (text_length / max_text_length) < 0.02:\n",
        "          print(f\"popped: {text_length} / {max_text_length} = {text_length / max_text_length}\")\n",
        "          child.extract()\n",
        "\n",
        "      return max_text_element, True\n",
        "\n",
        "    else:\n",
        "      return max_text_element, False"
      ],
      "metadata": {
        "id": "shVv6d81ALUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_main_content(html_content):\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "  main_content = []\n",
        "\n",
        "  current_tag = soup.body  # Start from the <body> tag\n",
        "\n",
        "  # Remove unnecessary elements\n",
        "  for child in current_tag.find_all(name=['script', 'template', 'figure', 'img', 'style', 'label', 'button', 'span']):\n",
        "    child.extract()\n",
        "\n",
        "  to_return = ''\n",
        "  stop = False\n",
        "  while not stop:\n",
        "    max_text_element, stop = find_max_text_element(current_tag)\n",
        "\n",
        "    # Move to the tag with the maximum text content\n",
        "    current_tag = max_text_element\n",
        "\n",
        "  return max_text_element.get_text()"
      ],
      "metadata": {
        "id": "N1Z5kCDai7KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example HTML content (replace this with your actual HTML content)\n",
        "html_content = open(r'/content/drive/MyDrive/raw_body.txt', 'r')\n",
        "\n",
        "main_content = extract_main_content(html_content)\n",
        "\n",
        "main_content = re.sub(r'<a .*?>', '', main_content)\n",
        "main_content = main_content.replace(\"</a>\",\"\")\n",
        "\n",
        "\n",
        "with open(r'/content/drive/MyDrive/test_1.txt', 'w') as text_file:\n",
        "  text_file.write(main_content)\n",
        "  text_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzSGdUQMi5Yv",
        "outputId": "65947cde-d5ef-4cd6-ac33-b9b84c21ced1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         1.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.01671429]\n",
            "[0.00109786 0.01390617 1.         0.00966113]\n",
            "[1.]\n",
            "[1.]\n",
            "[1.        0.1316051]\n",
            "In if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# newspaper\n",
        "\n",
        "God given library to scrape articles from the web"
      ],
      "metadata": {
        "id": "Gq-3Wotu9TYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install newspaper3k"
      ],
      "metadata": {
        "id": "3X94TqQ8wC-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "import re\n",
        "\n",
        "# Replace the URL with the actual URL of the article you want to scrape\n",
        "article_url = 'https://mariopeshev.com/4-ps-employee-relations-conflict-management/'\n",
        "\n",
        "# Create an Article object and download the article\n",
        "article = Article(article_url)\n",
        "article.download()\n",
        "\n",
        "# Parse the article content\n",
        "article.parse()\n",
        "\n",
        "# Get the output text and parse it with RegEx\n",
        "output_text = article.text\n",
        "# output_text = \"test1 . test2! . \\n\\n test1 . test2!\"\n",
        "\n",
        "# Use a regular expression to find consecutive duplicate content\n",
        "pattern = re.compile(r'(\\b.*?\\b[.!?])(?:[.\\n\\s]*)\\1')\n",
        "\n",
        "# Replace consecutive duplicate content with the first occurrence\n",
        "result = re.sub(pattern, r'\\1', output_text)\n",
        "\n",
        "# print(result)\n",
        "\n",
        "with open(r'/content/drive/MyDrive/test_2.txt', 'w') as text_file:\n",
        "  text_file.write(result)\n",
        "  text_file.close()"
      ],
      "metadata": {
        "id": "5QsifWVk768v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJxvo8L48SOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}